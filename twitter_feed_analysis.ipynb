{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install contractions\n",
    "# !pip install --upgrade azure-cognitiveservices-language-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import keys\n",
    "import requests\n",
    "import contractions\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "from azure.cognitiveservices.language.textanalytics import TextAnalyticsClient\n",
    "\n",
    "#from keys import subscription_key, text_analytics_base_url, bing_spell_check_key, spell_check_url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/azureuser/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:3: FutureWarning: from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "sample_tweets = pd.DataFrame.from_csv(\n",
    "    \"sample_tweets.csv\",\n",
    "    index_col=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Down ID</th>\n",
       "      <th>Posted time</th>\n",
       "      <th>text</th>\n",
       "      <th>Point latitiude</th>\n",
       "      <th>Point longitude</th>\n",
       "      <th>Bio location</th>\n",
       "      <th>Image url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"tag:search.twitter.com,2005:1109081183105835008\"</td>\n",
       "      <td>22/03/2019 13:15</td>\n",
       "      <td>\"Primeape was recently spotted trying to make ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"https://pbs.twimg.com/profile_images/76017773...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"tag:search.twitter.com,2005:1109080961277407234\"</td>\n",
       "      <td>22/03/2019 13:14</td>\n",
       "      <td>\"@gianninewbon Meet me at London Bridge at hal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\"Derbados\"</td>\n",
       "      <td>\"https://pbs.twimg.com/profile_images/10569615...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Down ID       Posted time  \\\n",
       "0  \"tag:search.twitter.com,2005:1109081183105835008\"  22/03/2019 13:15   \n",
       "1  \"tag:search.twitter.com,2005:1109080961277407234\"  22/03/2019 13:14   \n",
       "\n",
       "                                                text  Point latitiude  \\\n",
       "0  \"Primeape was recently spotted trying to make ...              NaN   \n",
       "1  \"@gianninewbon Meet me at London Bridge at hal...              NaN   \n",
       "\n",
       "   Point longitude Bio location  \\\n",
       "0              NaN          NaN   \n",
       "1              NaN   \"Derbados\"   \n",
       "\n",
       "                                           Image url  id  \n",
       "0  \"https://pbs.twimg.com/profile_images/76017773...   0  \n",
       "1  \"https://pbs.twimg.com/profile_images/10569615...   1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweets[\"id\"] = sample_tweets.index\n",
    "sample_tweets.rename(columns={\"Body text\": \"text\"}, inplace=True)\n",
    "#sample_tweets.text = sample_tweets.text.str.lower()\n",
    "sample_tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Down ID            44816\n",
       "Posted time        44816\n",
       "text               44816\n",
       "Point latitiude      701\n",
       "Point longitude      701\n",
       "Bio location       31946\n",
       "Image url          42978\n",
       "id                 44816\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_tweets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(sentence):\n",
    "    data = {'text': sentence}\n",
    "    params = {\n",
    "        'mkt':'en-us',\n",
    "        'mode':'proof'\n",
    "        }\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'Ocp-Apim-Subscription-Key': bing_spell_check_key,\n",
    "        }\n",
    "    response = requests.post(spell_check_url, headers=headers, params=params, data=data)\n",
    "    json_response = response.json()\n",
    "\n",
    "    for token in json_response[\"flaggedTokens\"]:\n",
    "        sentence = sentence.replace(str(token[\"token\"]), token[\"suggestions\"][0][\"suggestion\"])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset_tweets = sample_tweets[[\"id\", \"text\"]].sample(1000, random_state=1)\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "counter = 0\n",
    "def process_tweet(tweet):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter%1000 == 0:\n",
    "        print(\"Processing {} of {}\".format(counter, len(sample_tweets)))\n",
    "        print(datetime.now())\n",
    "\n",
    "    try:\n",
    "        normalized_tweet = tweet.strip()\n",
    "        if len(normalized_tweet) > 0:\n",
    "            normalized_tweet = spell_check(str(tweet)).lower().strip()\n",
    "            normalized_tweet = contractions.fix(normalized_tweet)\n",
    "            tokenized = tokenizer.tokenize(normalized_tweet)\n",
    "            # tokenized = nltk.word_tokenize(normalized_tweet)\n",
    "            # non_stop_words = [word for word in tokenized if (word not in stop_words)]\n",
    "            isascii = lambda s: len(s) == len(s.encode())\n",
    "            ascii_words = [word for word in tokenized[1:-1] if isascii(word)]\n",
    "            normalized_tweet = \" \".join(ascii_words)\n",
    "            return normalized_tweet\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as error_msg:\n",
    "        print(\"**********************\")\n",
    "        print(\"Tweet is {}\".format(tweet))\n",
    "        print(error_msg)\n",
    "        return None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started at 2019-06-25 15:26:08.478334\n"
     ]
    }
   ],
   "source": [
    "print(\"Process started at {}\".format(datetime.now()))\n",
    "sample_tweets[\"processed_text\"] = sample_tweets[\"text\"].apply(process_tweet)\n",
    "print(\"Process ended at {}\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.dropna(inplace=True, subset = [\"processed_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sample_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_str = sample_tweets.to_json(orient = \"records\")\n",
    "documents_json = json.loads(json.dumps({\"documents\" : json.loads(documents_str)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_json[\"documents\"][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_json[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_url = text_analytics_base_url + \"keyPhrases\"\n",
    "\n",
    "headers   = {\"Ocp-Apim-Subscription-Key\": subscription_key}\n",
    "response  = requests.post(keyphrase_url, headers=headers, json=documents_json)\n",
    "key_phrases = response.json()\n",
    "\n",
    "key_phrases_df = pd.DataFrame.from_dict(key_phrases['documents'])\n",
    "key_phrases_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(key_phrases_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  key_phrases_df[\"keyPhrases\"].apply(lambda x: len(x)).mean(),\n",
    "  key_phrases_df[\"keyPhrases\"].apply(lambda x: len(x)).max(),\n",
    "  key_phrases_df[\"keyPhrases\"].apply(lambda x: len(x)).min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_key_phrases(key_phrases):\n",
    "    if len(key_phrases) == 0:\n",
    "        return 1\n",
    "not_covered =  key_phrases_df[\"keyPhrases\"].apply(no_key_phrases).sum()\n",
    "\n",
    "print(\"Number of posts not covered by Key Phrases: {} or {} %\".format(\n",
    "    not_covered, \n",
    "    (not_covered/len(key_phrases_df)*100)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_long_key_phrases(keyPhrases):\n",
    "    for phrase in keyPhrases:\n",
    "        if len(phrase.split(\" \")) >  5:\n",
    "            keyPhrases.remove(phrase)\n",
    "    return keyPhrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases_df[\"shortKeyPhrases\"] = key_phrases_df[\"keyPhrases\"].apply(drop_long_key_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "  key_phrases_df[\"shortKeyPhrases\"].apply(lambda x: len(x)).mean(),\n",
    "  key_phrases_df[\"shortKeyPhrases\"].apply(lambda x: len(x)).max(),\n",
    "  key_phrases_df[\"shortKeyPhrases\"].apply(lambda x: len(x)).min()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases_df[key_phrases_df[\"shortKeyPhrases\"].map(len)>5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_key_phrases(row):\n",
    "    if len(row.shortKeyPhrases) == 0:\n",
    "        print(row.id)\n",
    "        return 1\n",
    "not_covered =  key_phrases_df.apply(no_key_phrases, axis=1).sum()\n",
    "\n",
    "print(\"Number of posts not covered by Shortened Key Phrases: {} or {} %\".format(\n",
    "    not_covered, \n",
    "    (not_covered/len(key_phrases_df)*100)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_tweets[sample_tweets.id==28909].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_key_phrases = set(sum(key_phrases_df[\"shortKeyPhrases\"].values, []))\n",
    "\n",
    "key_phrases_category = {}\n",
    "for key in unique_key_phrases:\n",
    "    key_phrases_category[key] = []\n",
    "\n",
    "def map_categories(row):\n",
    "    for phrase in row[\"shortKeyPhrases\"]:\n",
    "        key_phrases_category[phrase].append(row[\"id\"])\n",
    "    return None\n",
    "\n",
    "key_phrases_df.apply(map_categories, axis=1)\n",
    "\n",
    "key_phrases_category_count={}\n",
    "for key, value in key_phrases_category.items():\n",
    "    key_phrases_category_count[key] = len(value)\n",
    "\n",
    "from collections import OrderedDict\n",
    "key_phrases_category_count = OrderedDict(\n",
    "                               sorted(key_phrases_category_count.items(), \n",
    "                                      key=lambda kv: kv[1], \n",
    "                                      reverse=True)\n",
    "                            )\n",
    "\n",
    "print(len(key_phrases_category_count))\n",
    "# print(key_phrases_category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases_category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_phrases_df.id = key_phrases_df.id.astype(int)\n",
    "key_phrases_df.keyPhrases = key_phrases_df.keyPhrases.astype(str)\n",
    "key_phrases_df.shortKeyPhrases = key_phrases_df.shortKeyPhrases.astype(str)\n",
    "\n",
    "metadata_keyPhrases = pd.merge(\n",
    "    key_phrases_df[[\"id\", \"keyPhrases\", \"shortKeyPhrases\"]], \n",
    "    sample_tweets[[\"id\",\"text\", \"Posted time\", \"Bio location\"]], \n",
    "    on=\"id\", \n",
    "    how=\"inner\")\n",
    "\n",
    "metadata_keyPhrases.sort_values(by=[\"keyPhrases\", \"Bio location\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keyPhrases[[\"keyPhrases\", \"text\"]].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_keyPhrases.to_csv(\"tweet_keyPhrases.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
